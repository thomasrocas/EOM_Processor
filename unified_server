const express = require('express');
const { Client } = require('pg');
const multer = require('multer');
const fs = require('fs');
const path = require('path');
const { parse } = require('csv-parse');
const { stringify } = require('csv-stringify');
const copyFrom = require('pg-copy-streams').from;
const { Transform } = require('stream');

const app = express();
const PORT = 3000;

const client = new Client({
  user: 'postgres',
  host: 'localhost',
  database: 'clinicalvisits',
  password: '@DbAdmin@',
  port: 5432,
});

client.connect()
  .then(() => console.log('✅ Connected to PostgreSQL!'))
  .catch(err => console.error('❌ PostgreSQL connection error', err));

const upload = multer({ dest: 'uploads/' });
app.use(express.static('public'));
app.use(express.urlencoded({ extended: true }));

// ------------------ Helpers ------------------
function toNullIfEmpty(v) {
  if (v === null || v === undefined) return null;
  const s = String(v).trim();
  return s === '' ? null : s;
}

const CSV_OPTS_TOLERANT = {
  columns: true,
  trim: true,
  skip_empty_lines: true,
  relax_column_count: true,   // ✅ accept short/long rows
  relax_quotes: true,         // ✅ handle stray quotes
  bom: true,                  // ✅ handle UTF-8 BOM
  skip_lines_with_error: true // ✅ silently skip broken lines
};

// ------------------ ROUTES ------------------
app.post('/upload', upload.single('csvfile'), async (req, res) => {
  const tableName = req.body.tableName;
  const filePath = req.file?.path;

  if (!filePath) return res.status(400).send('No file uploaded.');
  if (!tableName) {
    fs.unlinkSync(filePath);
    return res.status(400).send('No table selected.');
  }

  try {
    if (tableName === 'casemanager') {
      await importCaseManager(filePath, tableName, res);
    } else if (tableName === 'maintable') {
      // CSV may include an "Action" column with values like "DELETE" to remove rows
      const summary = await importMainTable(filePath, tableName);
      return res.send(`
        ✅ MainTable Done!
        Processed Rows: ${summary.processedRows}
        Inserted/Updated: ${summary.insertedOrUpdated}
        Deleted Rows: ${summary.deletedCount}
        Skipped Rows: ${summary.skippedRows}
      `);
    } else if (tableName === 'unduplicatedpatients') {
      const summary = await importUnduplicatedPatients(filePath, tableName);
      return res.send(`
        ✅ UnduplicatedPatients Done!
        Processed Rows: ${summary.processedRows}
        Inserted/Updated: ${summary.insertedOrUpdated}
        Deleted Rows: ${summary.deletedCount}
        Skipped Rows: ${summary.skippedRows}
      `);
    } else if (tableName === 'queue_manager') {
      const summary = await importQueueManager(filePath, tableName);
      return res.send(`
        ✅ QueueManager Done!
        Processed Rows: ${summary.processedRows}
        Inserted/Updated: ${summary.insertedOrUpdated}
        Deleted Rows: ${summary.deletedCount}
        Skipped Rows: ${summary.skippedRows}
      `);
    } else {
      fs.unlinkSync(filePath);
      return res.status(400).send('❌ Unknown table selected.');
    }
  } catch (err) {
    console.error('❌ Server error:', err);
    if (fs.existsSync(filePath)) fs.unlinkSync(filePath);
    res.status(500).send(`<pre>Server error: ${err.message}</pre>`);
  }
});

// ------------------ CASEMANAGER IMPORT ------------------
async function importCaseManager(filePath, tableName, res) {
  try {
    const result = await client.query(`
      SELECT column_name, data_type
      FROM information_schema.columns
      WHERE table_schema = 'public' AND table_name = $1
      ORDER BY ordinal_position
    `, [tableName]);

    const tableColumns = result.rows.map(r => `"${r.column_name}"`);
    const columnTypes = {};
    result.rows.forEach(r => { columnTypes[r.column_name] = r.data_type; });

    await client.query(`TRUNCATE TABLE "${tableName}"`);

    function normalizeDate(v) {
      if (!v) return '';
      const d = new Date(v);
      return isNaN(d) ? '' : d.toISOString().split('T')[0];
    }

    const isDateCol = {};
    for (const [name, typ] of Object.entries(columnTypes)) {
      isDateCol[name] = /\bdate\b/i.test(typ);
    }

    const reorderAndSanitize = new Transform({
      writableObjectMode: true,
      readableObjectMode: true,
      transform(record, _enc, cb) {
        const out = [];
        for (const col of tableColumns) {
          const plain = col.replace(/(^")|("$)/g, '');
          let v = record[plain] ?? '';
          if (isDateCol[plain]) v = normalizeDate(v);
          out.push(v);
        }
        cb(null, out);
      }
    });

    const copyStream = client.query(copyFrom(
      `COPY "${tableName}" (${tableColumns.join(',')}) FROM STDIN CSV NULL ''`
    ));

    fs.createReadStream(filePath)
      .pipe(parse({ columns: true, trim: true }))
      .pipe(reorderAndSanitize)
      .pipe(stringify({ header: false }))
      .pipe(copyStream)
      .on('finish', () => {
        fs.unlinkSync(filePath);
        res.send(`✅ CaseManager: Table "${tableName}" imported successfully!`);
      })
      .on('error', err => { throw err; });

  } catch (err) {
    if (fs.existsSync(filePath)) fs.unlinkSync(filePath);
    throw err;
  }
}

// ------------------ MAINTABLE IMPORT ------------------
async function importMainTable(filePath, tableName) {
  const BATCH_SIZE = 500;
  let processedRows = 0, insertedOrUpdated = 0, deletedCount = 0;
  const skippedRows = [];
  const deleteIds = new Set();
  const csvVisitIds = new Set();
  let minDate = null, maxDate = null;
  let dateColumn = null;

  const result = await client.query(`
    SELECT column_name
    FROM information_schema.columns
    WHERE table_schema = 'public' AND table_name = $1 AND is_generated = 'NEVER'
    ORDER BY ordinal_position
  `, [tableName]);

  const allColumns = result.rows.map(r => r.column_name);
  const tableColumns = allColumns.filter(c => c !== 'Visit ID');
  dateColumn = allColumns.find(c => /date/i.test(c));
  await client.query('BEGIN');

    try {
      let batch = [];
      const parser = fs.createReadStream(filePath).pipe(parse(CSV_OPTS_TOLERANT));

      for await (const row of parser) {
        processedRows++;
        const visitId = row['Visit ID'];
        const action = (row['Action'] || '').toUpperCase();
        const dateStr = dateColumn ? row[dateColumn] : null;
        if (dateStr) {
          const d = new Date(dateStr);
          if (!isNaN(d)) {
            if (!minDate || d < minDate) minDate = d;
            if (!maxDate || d > maxDate) maxDate = d;
          }
        }

        if (!visitId) {
          skippedRows.push({ row, reason: 'Missing Visit ID' });
          continue;
        }

        csvVisitIds.add(visitId);

        if (action === 'DELETE') {
          deleteIds.add(visitId);
          continue; // Skip upsert for deletions
        }

        const rowData = tableColumns.map(c => toNullIfEmpty(row[c]));
        batch.push({ key: visitId, rowData });
        if (batch.length >= BATCH_SIZE) {
          await processBatch(batch, tableName, tableColumns, 'Visit ID');
          insertedOrUpdated += batch.length;
          batch = [];
        }
      }
      if (batch.length) {
        await processBatch(batch, tableName, tableColumns, 'Visit ID');
        insertedOrUpdated += batch.length;
      }

      if (deleteIds.size) {
        const deleteRes = await client.query(
          `DELETE FROM "${tableName}" WHERE "Visit ID" = ANY($1)`,
          [Array.from(deleteIds)]
        );
        deletedCount += deleteRes.rowCount;
      }

      if (dateColumn && minDate && maxDate && csvVisitIds.size) {
        const deleteExistingRes = await client.query(
          `DELETE FROM "${tableName}" WHERE "${dateColumn}" BETWEEN $1 AND $2 AND "Visit ID" NOT IN (SELECT unnest($3::text[]))`,
          [minDate, maxDate, Array.from(csvVisitIds)]
        );
        deletedCount += deleteExistingRes.rowCount;
      }

      await client.query('COMMIT');
      fs.unlinkSync(filePath);

      return {
        processedRows,
        insertedOrUpdated,
        deletedCount,
        skippedRows: skippedRows.length,
      };
    } catch (err) {
      await client.query('ROLLBACK');
      if (fs.existsSync(filePath)) fs.unlinkSync(filePath);
      throw err;
    }
  }

async function processBatch(batch, tableName, tableColumns, keyColumn) {
  const dedupedMap = new Map();
  for (const item of batch) dedupedMap.set(item.key, item.rowData);

  const dedupedBatch = Array.from(dedupedMap.entries()).map(([key, rowData]) => ({ key, rowData }));

  const values = [];
  const placeholders = dedupedBatch.map((b, i) => {
    const rowPlaceholders = b.rowData.map((_, j) => `$${i * (tableColumns.length + 1) + j + 2}`);
    values.push(b.key, ...b.rowData);
    return `($${i * (tableColumns.length + 1) + 1}, ${rowPlaceholders.join(', ')})`;
  });

  const colNames = [`"${keyColumn}"`, ...tableColumns.map(c => `"${c}"`)];
  const updateSet = tableColumns.map(c => `"${c}" = EXCLUDED."${c}"`);

  const query = `
    INSERT INTO "${tableName}" (${colNames.join(', ')})
    VALUES ${placeholders.join(', ')}
    ON CONFLICT ("${keyColumn}") DO UPDATE SET ${updateSet.join(', ')}
  `;
  await client.query(query, values);
}

// ------------------ UNDUPLICATEDPATIENTS IMPORT ------------------
async function importUnduplicatedPatients(filePath, tableName) {
  const BATCH_SIZE = 500;
  let processedRows = 0, insertedOrUpdated = 0, deletedCount = 0;
  const skippedRows = [];
  const deleteIds = new Set();
  const csvMrns = new Set();
  let minDate = null, maxDate = null;
  let dateColumn = null;

  const result = await client.query(`
    SELECT column_name
    FROM information_schema.columns
    WHERE table_schema = 'public' AND table_name = $1 AND is_generated = 'NEVER'
    ORDER BY ordinal_position
  `, [tableName]);

  const allColumns = result.rows.map(r => r.column_name);
  const tableColumns = allColumns.filter(c => c !== 'MRN');
  dateColumn = allColumns.find(c => /date/i.test(c));
  await client.query('BEGIN');

  try {
    let batch = [];
    const parser = fs.createReadStream(filePath).pipe(parse(CSV_OPTS_TOLERANT));

    for await (const row of parser) {
      processedRows++;
      const mrn = row['MRN'];
      const action = (row['Action'] || '').toUpperCase();
      const dateStr = dateColumn ? row[dateColumn] : null;
      if (dateStr) {
        const d = new Date(dateStr);
        if (!isNaN(d)) {
          if (!minDate || d < minDate) minDate = d;
          if (!maxDate || d > maxDate) maxDate = d;
        }
      }

      if (!mrn) {
        skippedRows.push({ row, reason: 'Missing MRN' });
        continue;
      }

      csvMrns.add(mrn);

      if (action === 'DELETE') {
        deleteIds.add(mrn);
        continue;
      }

      const rowData = tableColumns.map(c => toNullIfEmpty(row[c]));
      batch.push({ key: mrn, rowData });
      if (batch.length >= BATCH_SIZE) {
        await processBatch(batch, tableName, tableColumns, 'MRN');
        insertedOrUpdated += batch.length;
        batch = [];
      }
    }
    if (batch.length) {
      await processBatch(batch, tableName, tableColumns, 'MRN');
      insertedOrUpdated += batch.length;
    }

    if (deleteIds.size) {
      const deleteRes = await client.query(
        `DELETE FROM "${tableName}" WHERE "MRN" = ANY($1)`,
        [Array.from(deleteIds)]
      );
      deletedCount += deleteRes.rowCount;
    }

    if (dateColumn && minDate && maxDate && csvMrns.size) {
      const deleteExistingRes = await client.query(
        `DELETE FROM "${tableName}" WHERE "${dateColumn}" BETWEEN $1 AND $2 AND "MRN" NOT IN (SELECT unnest($3::text[]))`,
        [minDate, maxDate, Array.from(csvMrns)]
      );
      deletedCount += deleteExistingRes.rowCount;
    }

    await client.query('COMMIT');
    fs.unlinkSync(filePath);

    return {
      processedRows,
      insertedOrUpdated,
      deletedCount,
      skippedRows: skippedRows.length,
    };
  } catch (err) {
    await client.query('ROLLBACK');
    if (fs.existsSync(filePath)) fs.unlinkSync(filePath);
    throw err;
  }
}

// ------------------ QUEUE_MANAGER IMPORT ------------------
async function importQueueManager(filePath, tableName) {
  const BATCH_SIZE = 500;
  let processedRows = 0, insertedOrUpdated = 0, deletedCount = 0;
  const skippedRows = [];
  const deleteIds = new Set();
  const csvQueueIds = new Set();
  let minDate = null, maxDate = null;
  let dateColumn = null;

  const result = await client.query(`
    SELECT column_name
    FROM information_schema.columns
    WHERE table_schema = 'public' AND table_name = $1 AND is_generated = 'NEVER'
    ORDER BY ordinal_position
  `, [tableName]);

  const allColumns = result.rows.map(r => r.column_name);
  const keyColumn = 'Queue ID';
  const tableColumns = allColumns.filter(c => c !== keyColumn);
  dateColumn = allColumns.find(c => /date/i.test(c));
  await client.query('BEGIN');

  try {
    let batch = [];
    const parser = fs.createReadStream(filePath).pipe(parse(CSV_OPTS_TOLERANT));

    for await (const row of parser) {
      processedRows++;
      const queueId = row[keyColumn];
      const action = (row['Action'] || '').toUpperCase();
      const dateStr = dateColumn ? row[dateColumn] : null;
      if (dateStr) {
        const d = new Date(dateStr);
        if (!isNaN(d)) {
          if (!minDate || d < minDate) minDate = d;
          if (!maxDate || d > maxDate) maxDate = d;
        }
      }

      if (!queueId) {
        skippedRows.push({ row, reason: `Missing ${keyColumn}` });
        continue;
      }

      csvQueueIds.add(queueId);

      if (action === 'DELETE') {
        deleteIds.add(queueId);
        continue;
      }

      const rowData = tableColumns.map(c => toNullIfEmpty(row[c]));
      batch.push({ key: queueId, rowData });
      if (batch.length >= BATCH_SIZE) {
        await processBatch(batch, tableName, tableColumns, keyColumn);
        insertedOrUpdated += batch.length;
        batch = [];
      }
    }
    if (batch.length) {
      await processBatch(batch, tableName, tableColumns, keyColumn);
      insertedOrUpdated += batch.length;
    }

    if (deleteIds.size) {
      const deleteRes = await client.query(
        `DELETE FROM "${tableName}" WHERE "${keyColumn}" = ANY($1)`,
        [Array.from(deleteIds)]
      );
      deletedCount += deleteRes.rowCount;
    }

    if (dateColumn && minDate && maxDate && csvQueueIds.size) {
      const deleteExistingRes = await client.query(
        `DELETE FROM "${tableName}" WHERE "${dateColumn}" BETWEEN $1 AND $2 AND "${keyColumn}" NOT IN (SELECT unnest($3::text[]))`,
        [minDate, maxDate, Array.from(csvQueueIds)]
      );
      deletedCount += deleteExistingRes.rowCount;
    }

    await client.query('COMMIT');
    fs.unlinkSync(filePath);

    return {
      processedRows,
      insertedOrUpdated,
      deletedCount,
      skippedRows: skippedRows.length,
    };
  } catch (err) {
    await client.query('ROLLBACK');
    if (fs.existsSync(filePath)) fs.unlinkSync(filePath);
    throw err;
  }
}

// ------------------ ROOT ------------------
app.get('/', (req, res) => {
  res.sendFile(path.join(__dirname, 'public', 'index.html'));
});

app.listen(PORT, () => console.log(`🚀 Unified server running at http://localhost:${PORT}`));
